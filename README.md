# LLM-From-Scratch
I build every core component of large language model (LLM) architectures from scratch, following Sebastian Raschka's book â€” covering everything from data preparation and multi-head self-attention modules to classification and instruction fine-tuning of open-source models and deployed it on AWS Sagemaker!

# Additional Concepts Implemented

## KV Caching

![KV Speed Comparison Diagram](Pretraining-with-KV-Caching/kv_cache_performance_comparison.png)

### ğŸ” In *normal attention* (used in **training**):
- For each token `tâ‚, tâ‚‚, ..., tâ‚œ`, you recompute:
  - Query `qáµ¢`, Key `káµ¢`, and Value `váµ¢`
- You build full `Q`, `K`, and `V` matrices **fresh** every time
- Compute all attention scores, softmax weights, and context vectors `[câ‚, câ‚‚, ..., câ‚œ]` in **parallel** using full sequences (because we know all tokens ahead of time)

### âš¡ In *KV caching* (used in **inference/generation**):
- We **generate one token at a time** (we donâ€™t know the next tokens yet)
- So we donâ€™t recompute all `K` and `V` at each step â€” we **cache** (store) the previous ones to avoid redundant computation

### ğŸ§± Step-by-Step Example

**Sentence generation: `"The cat sat on"`**

Now you're generating the next token: `"the"`

### ğŸŸ¡ Without KV Caching (Normal Inference)
At token `tâ‚… = "the"`:
- Recompute:
  - `kâ‚, kâ‚‚, kâ‚ƒ, kâ‚„, kâ‚…`
  - `vâ‚, vâ‚‚, vâ‚ƒ, vâ‚„, vâ‚…`
  - `qâ‚…`
- Compute:
  - `qâ‚… Ã— [kâ‚, kâ‚‚, kâ‚ƒ, kâ‚„, kâ‚…]áµ€` â†’ attention scores
  - Softmax â†’ context vector `câ‚…`

âŒ Inefficient â€” recomputing everything at each step!

### ğŸŸ¢ With KV Caching (Efficient Inference)

At token `tâ‚… = "the"`:
- Already stored:
  - `kâ‚..â‚„`, `vâ‚..â‚„`
- Compute:
  - `qâ‚…`, `kâ‚…`, `vâ‚…`
- Update caches:
  ```text
  K_cache = [kâ‚, ..., kâ‚„] â†’ [kâ‚, ..., kâ‚…]
  V_cache = [vâ‚, ..., vâ‚„] â†’ [vâ‚, ..., vâ‚…]
- Compute attention scores -> attention weights -> single row of context vector -> NN architecture -> softmax -> probabilities.
